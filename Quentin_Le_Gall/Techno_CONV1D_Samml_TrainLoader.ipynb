{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import os\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_plot import hdr_plot_style\n",
    "hdr_plot_style()\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TechnoDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dat_location,\n",
    "                 size=2**15) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.samples = np.memmap(\n",
    "            dat_location,\n",
    "            dtype=\"float32\",\n",
    "            mode=\"r\",\n",
    "        )\n",
    "        self.samples = self.samples[:size * (len(self.samples) // size)]\n",
    "        self.samples = self.samples.reshape(-1, 1, size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.from_numpy(np.copy(self.samples[index])).float()\n",
    "\n",
    "dataset = TechnoDataset(\"./data/TECHNO/techno_resampled.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, encoder, decoder, encoding_dim):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoding_dims = encoding_dim\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(AE):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, encoding_dims, latent_dims):\n",
    "        super(VAE, self).__init__(encoder, decoder, encoding_dims)\n",
    "        self.latent_dims = latent_dims\n",
    "        self.mu = nn.Sequential(nn.Linear(self.encoding_dims, self.latent_dims))\n",
    "        self.sigma = nn.Sequential(nn.Linear(self.encoding_dims, self.latent_dims), nn.Softplus())\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \n",
    "        x = self.encoder(x)\n",
    "        mu = self.mu(x)\n",
    "        sigma = self.sigma(x)\n",
    "        \n",
    "        return mu, sigma\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode the inputs\n",
    "        z_params = self.encode(x)\n",
    "        # Obtain latent samples and latent loss\n",
    "        z_tilde, kl_div = self.latent(x, z_params)\n",
    "        # Decode the samples\n",
    "        x_tilde = self.decode(z_tilde)\n",
    "        return x_tilde.reshape(-1, 1, 28, 28), kl_div\n",
    "    \n",
    "    def latent(self, x, z_params):\n",
    "        \n",
    "        mu, sigma = z_params\n",
    "\n",
    "        var = sigma * sigma\n",
    "        log_var = torch.log(var)\n",
    "\n",
    "        z = torch.rand_like(mu) * sigma + mu\n",
    "        kl_div = torch.sum(mu * mu + var - log_var - 1)\n",
    "\n",
    "        return z, kl_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(torch.nn.Module):\n",
    "    def __init__(self, outer_shape):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.outer_shape = outer_shape\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), *self.outer_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_encoder_decoder(nin=1, n_latent = 16, n_hidden = 64, n_params = 0, n_classes = 1):\n",
    "    # Encoder network\n",
    "    encoder = nn.Sequential(\n",
    "          nn.Conv1d(nin, n_hidden, 4,  padding=1), nn.ReLU(),\n",
    "          nn.Conv1d(n_hidden, 2*n_hidden, 4,  padding=1), nn.ReLU(),\n",
    "          nn.Flatten(),\n",
    "          nn.Linear(7*7*2*n_hidden, 1024),nn.ReLU(),\n",
    "          nn.Linear(1024, n_latent)#, nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    # Decoder network\n",
    "    decoder = nn.Sequential(\n",
    "          nn.Linear(n_latent,1024), nn.ReLU(),\n",
    "          nn.Linear(1024, 7*7*2*n_hidden), nn.ReLU(),\n",
    "          Reshape((2*n_hidden,7,7,)),\n",
    "          nn.ConvTranspose1d(2*n_hidden, n_hidden, 4,  padding=1), nn.ReLU(),\n",
    "          nn.ConvTranspose1d(n_hidden, nin*n_classes, 4,  padding=1), nn.Sigmoid()\n",
    "        )\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33502208"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#o33552384 - (7*7*2*n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction criterion\n",
    "recons_criterion = torch.nn.BCELoss(reduction='sum')\n",
    "\n",
    "def compute_loss(model, x):\n",
    "    model = model#.to(device)    \n",
    "    x = x#.to(device)    \n",
    "    beta = 0.1\n",
    "    y_pred,kl_div = model(x)\n",
    "    recons_loss = recons_criterion(y_pred, x)\n",
    "    full_loss = recons_loss + kl_div*beta\n",
    "    #print(kl_div,recons_loss,full_loss)\n",
    "    \n",
    "    return full_loss,kl_div,recons_loss\n",
    "\n",
    "def train_step(model, x, optimizer):\n",
    "    model = model#.to(device)\n",
    "    x = x#.to(device)\n",
    "    # Compute the loss.\n",
    "    loss,kl_div,recons_loss = compute_loss(model, x)\n",
    "    # Before the backward pass, zero all of the network gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass: compute gradient of the loss with respect to parameters\n",
    "    loss.backward()\n",
    "    # Calling the step function to update the parameters\n",
    "    optimizer.step()\n",
    "    return loss,kl_div,recons_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = './data'\n",
    "valid_ratio = 0.999999\n",
    "# Load the dataset for the training/validation sets\n",
    "train_valid_dataset =  dataset\n",
    "# Split it into training and validation sets\n",
    "nb_train = int((1.0 - valid_ratio) * len(train_valid_dataset) +1)\n",
    "nb_valid =  int(valid_ratio * len(train_valid_dataset))\n",
    "\n",
    "train_dataset, valid_dataset = torch.utils.data.dataset.random_split(train_valid_dataset, [nb_train, nb_valid])\n",
    "\n",
    "\n",
    "# Prepare \n",
    "num_threads = 0     # Loading the dataset is using 4 CPU threads\n",
    "batch_size  = 128   # Using minibatches of 128 samples\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_threads)\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_threads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Bernoulli or Multinomial loss\n",
    "num_classes = 1\n",
    "# Number of hidden and latent\n",
    "n_hidden = 512\n",
    "n_latent = 2\n",
    "# Compute input dimensionality\n",
    "nin = 1\n",
    "# Construct encoder and decoder\n",
    "encoder, decoder = construct_encoder_decoder(nin, n_hidden = n_hidden, n_latent = n_latent, n_classes = num_classes)\n",
    "encoder = encoder#.to(device)\n",
    "decoder = decoder#.to(device)\n",
    "# Build the VAE model\n",
    "model = VAE(encoder, decoder, n_hidden, n_latent)#.to(device)\n",
    "# Construct the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_vae(model, epochs):\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        full_loss = torch.Tensor([0])#.to(device)\n",
    "        kl_div = torch.Tensor([0])#.to(device)\n",
    "        recons_loss = torch.Tensor([0])#.to(device)\n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        for i, x in enumerate(train_loader):\n",
    "            #x=x#.to(device)\n",
    "            #data_loader = data_loader#.to(device)\n",
    "            #model=model#.to(device)\n",
    "            \n",
    "            full_loss_add,kl_div_add,recons_loss_add = train_step(model, x, optimizer)\n",
    "            full_loss += full_loss_add\n",
    "            kl_div += kl_div_add\n",
    "            recons_loss += recons_loss_add\n",
    "            \n",
    "\n",
    "        print('Epoch: {}, Test set ELBO: {}, Kl : {}, recons :{}'.format(epoch, full_loss,kl_div,recons_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x33552384 and 50176x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [60], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m epochs\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m\n\u001b[1;32m----> 3\u001b[0m train_vae(model,epochs)\n",
      "Cell \u001b[1;32mIn [59], line 13\u001b[0m, in \u001b[0;36mtrain_vae\u001b[1;34m(model, epochs)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39m# Forward pass: compute predicted y by passing x to the model.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      9\u001b[0m     \u001b[39m#x=x#.to(device)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[39m#data_loader = data_loader#.to(device)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m#model=model#.to(device)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     full_loss_add,kl_div_add,recons_loss_add \u001b[39m=\u001b[39m train_step(model, x, optimizer)\n\u001b[0;32m     14\u001b[0m     full_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m full_loss_add\n\u001b[0;32m     15\u001b[0m     kl_div \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m kl_div_add\n",
      "Cell \u001b[1;32mIn [56], line 19\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, x, optimizer)\u001b[0m\n\u001b[0;32m     17\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m#.to(device)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m# Compute the loss.\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m loss,kl_div,recons_loss \u001b[39m=\u001b[39m compute_loss(model, x)\n\u001b[0;32m     20\u001b[0m \u001b[39m# Before the backward pass, zero all of the network gradients\u001b[39;00m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn [56], line 8\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(model, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m#.to(device)    \u001b[39;00m\n\u001b[0;32m      7\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m----> 8\u001b[0m y_pred,kl_div \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      9\u001b[0m recons_loss \u001b[39m=\u001b[39m recons_criterion(y_pred, x)\n\u001b[0;32m     10\u001b[0m full_loss \u001b[39m=\u001b[39m recons_loss \u001b[39m+\u001b[39m kl_div\u001b[39m*\u001b[39mbeta\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [53], line 22\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# Encode the inputs\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     z_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(x)\n\u001b[0;32m     23\u001b[0m     \u001b[39m# Obtain latent samples and latent loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     z_tilde, kl_div \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlatent(x, z_params)\n",
      "Cell \u001b[1;32mIn [53], line 11\u001b[0m, in \u001b[0;36mVAE.encode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[0;32m     12\u001b[0m     mu \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmu(x)\n\u001b[0;32m     13\u001b[0m     sigma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigma(x)\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\quent\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x33552384 and 50176x1024)"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "\n",
    "train_vae(model,epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2823d83d0138f4e88bebb1ac6893599dadbc35aa0bdabe1fb606adea8faab8b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
